{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceca3d77",
   "metadata": {},
   "source": [
    "1. 웹스크래핑 연습문제 \n",
    "1-1 Daum 뉴스기사 제목 스크래핑하기 \n",
    "질문 1. 아래의 url에서 뉴스기사의 링크와 제목을 출력하세요 \n",
    " # 다음 경제 뉴스 URL\n",
    "    url = 'https://news.daum.net/economy'\n",
    "\n",
    "    파일명: 01_news_scraping.ipynb\n",
    "       response를 utf-8로 encoding\n",
    "<ul>\n",
    "   <li> \n",
    "     <a href=\"링크\">\n",
    "     <div class=\"cont_thum\">\n",
    "        <strong class=\"tit_txt\">뉴스제목\n",
    "\n",
    "for li_tag in select('ul.list_newsheadline2 li'):\n",
    "    a_tag = li_tag.find('a')\n",
    "    link = a_tag['href']\t\n",
    "        \n",
    "    strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "    title = strong_tag.text\n",
    "\n",
    "\n",
    "    실행결과: \n",
    "https://news.daum.net/economy\n",
    "<class 'requests.models.Response'>\n",
    "200\n",
    "<class 'bs4.element.ResultSet'> 9\n",
    "https://v.daum.net/v/20250408113412211\n",
    "트럼프 때문에 '공동 묘지' 된 금융시장, 투자 전문가 \"뉴스 보지 말라\" 왜?\n",
    "https://v.daum.net/v/20250408111800386\n",
    "한국 GDP 성장 기여도 95% 달하는 수출에 직격탄 [관세전쟁 후폭풍]\n",
    "https://v.daum.net/v/20250408111603311\n",
    "'검은 월요일' 1987년과 2025년\n",
    "https://v.daum.net/v/20250408111500208\n",
    "주왕산국립공원 1/3 삼킨 화마…1대뿐인 헬기 무용지물이었다\n",
    "https://v.daum.net/v/20250408110625822\n",
    "\"기금형 퇴직연금, 국민연금과 달라…사각지대 근로자 품어야\"[이슈인터뷰]\n",
    "https://v.daum.net/v/20250408110307619\n",
    "세수결손에 나라살림 104.8조 적자…GDP 대비 4% 넘어\n",
    "https://v.daum.net/v/20250408110021362\n",
    "지난해 국가채무 1175조로 ‘역대 최대’인데… 정부 “예상보다 덜 늘어”\n",
    "https://v.daum.net/v/20250408100455408\n",
    "올 2월에 첫 월세 60%? 5년 전에도 '월세 시대'였다 [분석+]\n",
    "https://v.daum.net/v/20250408093512932\n",
    "K-관광 로드쇼, '국교 정상화 60주년' 일본인 역대 최다 방한 이룬다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf36935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 True\n",
      "<class 'requests.models.Response'>\n",
      "<Response [200]>\n",
      " scraping Daum Economy News Headlines scraping\n",
      "\n",
      "1. “스포츠 감성 담았다”…‘9.58초 전설’ 육상화가 ‘스트리트 패션’ 상징으로\n",
      "   - 링크: https://v.daum.net/v/20250723074847881\n",
      "\n",
      "2. 인니, 관세 낮추려 美 농산물·車 규제 완화…경제안보도 협력(종합2보)\n",
      "   - 링크: https://v.daum.net/v/20250723073950756\n",
      "\n",
      "3. [스타트경제] \"소비쿠폰 싸게 팝니다\"...정부 \"적발 시 전액 환수·형사 처벌\"\n",
      "   - 링크: https://v.daum.net/v/20250723072813583\n",
      "\n",
      "4. 서민 대출 창구 더 좁아지나…저축은행 중금리대출 5.3% ↓\n",
      "   - 링크: https://v.daum.net/v/20250723072526546\n",
      "\n",
      "5. 대출규제 정책, 일관되게 밀고나가 ‘부동산 불패’ 끝내야\n",
      "   - 링크: https://v.daum.net/v/20250723070652276\n",
      "\n",
      "6. aT사장 \"기후변화 대응전략 필요…식량안보 차원서 접근해야\"\n",
      "   - 링크: https://v.daum.net/v/20250723070443240\n",
      "\n",
      "7. 투명인간 취급 당한 노동자들 “응답하라, 정치여~”\n",
      "   - 링크: https://v.daum.net/v/20250723070439237\n",
      "\n",
      "8. 29CM 이구홈 성수, 연 20조원 라이프스타일 시장 판도 바꾼다\n",
      "   - 링크: https://v.daum.net/v/20250723070126093\n",
      "\n",
      "9. 혁신 생태계 발목잡는 스톡옵션 과세특례…\"제도 정비 이뤄져야\" [광장의 조세]\n",
      "   - 링크: https://v.daum.net/v/20250723070054084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#1.스크패링하려는 URL과 헤더 정보 설정\n",
    "url = 'https://news.daum.net/economy'\n",
    "#Useragent 설정해주기: 로봇이 아닌 브라우저가 요청하는 것처럼\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\"\n",
    "}\n",
    "#HTTP 요청 보내기\n",
    "response = requests.get(url,headers=headers)\n",
    "#요청이 성공했는지 확인(상태코드가 200 OK)\n",
    "# 인코딩 방식을 UTF-8로 강제 설정\n",
    "response.encoding = 'utf-8'\n",
    "print(response.status_code, response.ok)\n",
    "print(type(response))\n",
    "print(response)\n",
    "\n",
    "if response.ok: \n",
    "    #Beautifulsoup객체로 변환\n",
    "    html=response.text\n",
    "    soup=BeautifulSoup(html, 'html.parser')\n",
    "    #CSS선택자를 사용해 원하는 요소 찾기\n",
    "    #뉴스메인의 제목(<li>태그)모두 찾기\n",
    "    li_tags = soup.select('ul.list_newsheadline2 li')\n",
    "    #list comprehension으로 제목과 링크만 뽑아 리스트 만들기\n",
    "    news_list = []\n",
    "    # 5. 찾은 <li> 태그들을 하나씩 돌면서 제목과 링크 추출\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        strong_tag = li.select_one('strong.tit_txt')\n",
    "\n",
    "        # a_tag와 strong_tag가 모두 존재할 때만 데이터를 추출\n",
    "        if a_tag and strong_tag:\n",
    "            link = a_tag['href']\n",
    "            title = strong_tag.text.strip()\n",
    "            \n",
    "            news_list.append({\n",
    "                'title': title,\n",
    "                'link': link\n",
    "            })\n",
    "            \n",
    "    # 6. 가공된 news_list를 기반으로 화면에 출력\n",
    "    print(\" scraping Daum Economy News Headlines scraping\\n\")\n",
    "    for index, news in enumerate(news_list, 1):\n",
    "        print(f\"{index}. {news['title']}\")\n",
    "        print(f\"   - 링크: {news['link']}\\n\")\n",
    "\n",
    "\n",
    "else:\n",
    "    # 요청이 실패했을 경우\n",
    "    print(f\"Error: Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f83c3",
   "metadata": {},
   "source": [
    "질문2:  여러개의 section 중 하나를 선택해서 url에서 뉴스기사의 링크와 제목을 출력하는 코드를 함수로 작성하기\n",
    "    \n",
    "    # 경제 뉴스 URL\n",
    "    url = 'https://news.daum.net/economy'\n",
    "    # 사회 뉴스 URL\n",
    "    url = 'https://news.daum.net/society'\n",
    "\n",
    "    파일명: 01_news_scraping.ipynb\n",
    "    코드설명\n",
    "           print_news() 함수에서 \n",
    "      section name (섹션명)을 입력 받아서 section_dict에서 section 영문   \n",
    "      문자열을 추출해서 url을 생성합니다.\n",
    "      그 이후의 코드는 질문1번 문제와 동일합니다.\n",
    "\n",
    "section_dict = {'기후/환경':'climate','사회':'society','경제':'economy','정치':'politics',\\\n",
    "             '국제':'world','문화':'culture','생활':'life','IT/과학':'tech','인물':'people'}\n",
    "\n",
    "# 함수선언\n",
    "def print_news(section_name):\n",
    "\tpass\n",
    "\n",
    "# 함수호출\n",
    "print_news('경제')\n",
    "\tprint_news('사회')\n",
    "\n",
    "    실행결과: \n",
    "      print_news('경제')\n",
    "======> https://news.daum.net/economy 경제 뉴스 <======\n",
    "https://v.daum.net/v/20250408121010759\n",
    "\"아무도 대신 챙겨주지 않습니다\" 퇴직금으론 부족한 은퇴 후 삶 \"개인 연금 중요\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61db92da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========> https://news.daum.net/economy 경제 뉴스 <========\n",
      "https://v.daum.net/v/20250723083141756\n",
      "\"기아 PV5, '다품종 소량생산' 최초 도전… 어떻게 가능할까?\"\n",
      "https://v.daum.net/v/20250723082949708\n",
      "\"임시방편일까, 민생회복 마중물 될까··· 소비쿠폰 둘러싼 Q&A\"\n",
      "https://v.daum.net/v/20250723074847881\n",
      "\"“스포츠 감성 담았다”…‘9.58초 전설’ 육상화가 ‘스트리트 패션’ 상징으로\"\n",
      "https://v.daum.net/v/20250723073950756\n",
      "\"인니, 관세 낮추려 美 농산물·車 규제 완화…경제안보도 협력(종합2보)\"\n",
      "https://v.daum.net/v/20250723072813583\n",
      "\"[스타트경제] \"소비쿠폰 싸게 팝니다\"...정부 \"적발 시 전액 환수·형사 처벌\"\"\n",
      "https://v.daum.net/v/20250723072526546\n",
      "\"서민 대출 창구 더 좁아지나…저축은행 중금리대출 5.3% ↓\"\n",
      "https://v.daum.net/v/20250723070652276\n",
      "\"대출규제 정책, 일관되게 밀고나가 ‘부동산 불패’ 끝내야\"\n",
      "https://v.daum.net/v/20250723070443240\n",
      "\"aT사장 \"기후변화 대응전략 필요…식량안보 차원서 접근해야\"\"\n",
      "https://v.daum.net/v/20250723070439237\n",
      "\"투명인간 취급 당한 노동자들 “응답하라, 정치여~”\"\n",
      "\n",
      "========> https://news.daum.net/society 사회 뉴스 <========\n",
      "https://v.daum.net/v/20250723082548622\n",
      "\"\"소비쿠폰, 배달앱도 가능\" 치킨 한 마리 '공짜'까지…알뜰 사용 팁은\"\n",
      "https://v.daum.net/v/20250723080647210\n",
      "\"공영방송 정상화 꼬인 실타래 풀려면\"\n",
      "https://v.daum.net/v/20250723080146089\n",
      "\"'4일장 해소' 서울시 17년 전 설계에 답 있었다…증설 화장로 4기 내달 가동\"\n",
      "https://v.daum.net/v/20250723080008058\n",
      "\"폭염 이어 엎친 데 덮친 농가 피해… 밥상 물가도 직격탄\"\n",
      "https://v.daum.net/v/20250723075748019\n",
      "\"경북도·경주시, 2025년 APEC 정상회의 D-100일 ‘준비 순항’\"\n",
      "https://v.daum.net/v/20250723075102917\n",
      "\"“유령도시가 컨셉인가”...수백억 투입한 테마거리, 사람 찾기가 어렵다\"\n",
      "https://v.daum.net/v/20250723070203153\n",
      "\"비행기표보다 비싼 제주 렌터카?…대여요금 대폭 손질\"\n",
      "https://v.daum.net/v/20250723070150127\n",
      "\"의총협, 본과 3학년 대학별 '자율 졸업' 검토···의대 정상화 가닥\"\n",
      "https://v.daum.net/v/20250723070143110\n",
      "\"코로나 이후 도-농 학업 격차 커져…읍면 국·영 기초학력 미달 3배↑\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 문제에 제시된 딕셔너리\n",
    "section_dict = {\n",
    "    '기후/환경': 'climate', '사회': 'society', '경제': 'economy', '정치': 'politics',\n",
    "    '국제': 'world', '문화': 'culture', '생활': 'life', 'IT/과학': 'tech', '인물': 'people'\n",
    "}\n",
    "# 뉴스 스크래핑 및 출력을 담당하는 함수\n",
    "def print_news(section_name):\n",
    "    \"\"\"\n",
    "    한글 섹션 이름을 입력받아 해당 Daum 뉴스 페이지의\n",
    "    기사 제목과 링크를 스크래핑하여 출력하는 함수\n",
    "    \"\"\"\n",
    "    # 1. 입력받은 한글 섹션 이름으로 딕셔너리에서 영어 이름 찾기\n",
    "    english_section = section_dict.get(section_name)\n",
    "\n",
    "    # 만약 딕셔너리에 없는 이름이 들어오면 함수 종료\n",
    "    if not english_section:\n",
    "        print(f\"'{section_name}'은(는) 유효한 섹션 이름이 아닙니다.\")\n",
    "        return\n",
    "    # 2. 찾은 영어 이름을 사용해 정확한 URL 만들기\n",
    "    url = f'https://news.daum.net/{english_section}'\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # 3. 웹사이트에 HTTP 요청 보내기\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.encoding = 'utf-8'\n",
    "    # 4. 요청 성공 시 스크래핑 시작\n",
    "    if response.ok:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # <li> 태그들을 모두 찾기\n",
    "        li_tags = soup.select('ul.list_newsheadline2 li')\n",
    "        \n",
    "        news_list = []\n",
    "        # 찾은 <li> 태그들을 하나씩 돌면서 제목과 링크 추출\n",
    "        for li in li_tags:\n",
    "            a_tag = li.find('a')\n",
    "            strong_tag = li.select_one('strong.tit_txt')\n",
    "\n",
    "            if a_tag and strong_tag:\n",
    "                link = a_tag['href']\n",
    "                title = strong_tag.text.strip()\n",
    "                \n",
    "                news_list.append({\n",
    "                    'title': title,\n",
    "                    'link': link\n",
    "                })\n",
    "\n",
    "        # 5. 지정된 형식에 맞춰 결과 출력\n",
    "        print(f\"\\n========> {url} {section_name} 뉴스 <========\")\n",
    "        if not news_list:\n",
    "            print(\"뉴스를 찾을 수 없습니다.\")\n",
    "        \n",
    "        for news in news_list:\n",
    "            print(news['link'])\n",
    "            print(f'\"{news[\"title\"]}\"')\n",
    "    else:\n",
    "        print(f\"Error: 페이지를 가져오지 못했습니다. 상태 코드: {response.status_code}\")\n",
    "\n",
    "\n",
    "# 함수 호출\n",
    "print_news('경제')\n",
    "print_news('사회')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015326db",
   "metadata": {},
   "source": [
    "2-1. Nate 뉴스기사 제목 스크래핑하기 (필수)\n",
    "https://news.nate.com/recent?mid=n0100\n",
    "최신뉴스, 정치 , 경제, 사회, 세계, IT/과학 \n",
    "6개의 섹션의 뉴스를 출력하는 함수를 생성하여 스크래핑 하기\n",
    "\n",
    "Image, 기사제목, 기사링크\n",
    "\n",
    "뉴스기사의 Image를 출력 하세요 \n",
    "1) Image의 절대경로와 상대 경로를 합치려면 urljoin 함수를 사용하세요.\n",
    "    from urllib.parse import urljoin\n",
    "\n",
    "2) Image 출력은 Image 클래스와 display 함수를 사용하세요.\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "3) img 엘리먼트의 존재 여부를 체크하신 후에 src 속성의 이미지를 경로를 추출하기\n",
    "  => Image 가 없는 뉴스도 있기 때문에 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecc22887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin # 상대경로 URL을 절대경로로 바꿔주기 위함\n",
    "from IPython.display import Image, display # Jupyter/Colab 환경에서 이미지 출력을 위함\n",
    "# 1. 섹션 이름과 URL의 mid 값을 딕셔너리로 정의\n",
    "nate_section_mid = {\n",
    "    '최신뉴스': 'n0100',\n",
    "    '정치': 'n0200',\n",
    "    '경제': 'n0300',\n",
    "    '사회': 'n0400',\n",
    "    '세계': 'n0500',\n",
    "    'IT/과학': 'n0600'\n",
    "}\n",
    "def scrape_nate_news(section_name):\n",
    "    \"\"\"\n",
    "    네이트 뉴스의 특정 섹션 페이지에서\n",
    "    기사 이미지, 제목, 링크를 스크래핑하는 함수\n",
    "    \"\"\"\n",
    "    mid = nate_section_mid.get(section_name)\n",
    "    if not mid:\n",
    "        print(f\"'{section_name}'은(는) 유효한 섹션 이름이 아닙니다.\")\n",
    "        return\n",
    "\n",
    "    # 베이스 URL과 최종 URL 정의\n",
    "    base_url = \"https://news.nate.com/recent?mid=n0100\"\n",
    "    url = f\"{base_url}/recent?mid={mid}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    print(f\"\\n========> {url} {section_name} 뉴스 <========\")\n",
    "\n",
    "    if not response.ok:\n",
    "        print(f\"Error: 페이지를 가져오지 못했습니다. 상태 코드: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
